from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from vector import retriever

from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, ContextTypes, MessageHandler, filters

TELEGRAM_TOKEN=""

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¢–ü ü§ñ")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    question = update.message.text
    answers = retriever.invoke(question)
    result = chain.invoke({"answers": answers, "question": question})
    await update.message.reply_text(result)


model = OllamaLLM(model="deepseek-r1:14b")

template = """
–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –≤ –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ —É—Å–ª—É–≥–∞—Ö –∂–∫—Ö (–≥–æ—Ä—è—á–∞—è –≤–æ–¥–∞, —ç–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏—è)

–ó–¥–µ—Å—å —Ñ–∞–∫—Ç—ã –æ–± –æ—Ç–≤–µ—Ç–∞—Ö –Ω–∞ —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã: {answers}

–ó–¥–µ—Å—å –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

if __name__ == "__main__":
    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    print("–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω")
    app.run_polling()